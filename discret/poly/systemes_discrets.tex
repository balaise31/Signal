\chapter{Systèmes discrets}
\def\y{\vec{y}}
\def\x{\vec{x}}
\def\h{\vec{h}}
\def\g{\vec{g}}
\def\d0{\vec{\delta_0}}
\def\xu{\vec{x_1}}
\def\xd{\vec{x_2}}


L'analogie entre systèmes continus et discrets est forte, nous
proposons ici de revenir sur les systèmes continus et d'aborder les
systèmes discrets avec une approche commune par calcul opérationnel.


\imagetexte{0.3}{op_sys}{0.6}{ Un système (conintu ou discret) est vu
  comme un opérateur G transformant un signal $\x$ d'un espace de
  signaux $E$ en un signal $\y$ du même espace. L'automaticienne
  associera un schéma-bloc (en rose les signaux et espaces de signaux,
  en noir les systèmes) à cet opérateur transformant un signal en
  signal.}

Nous nous limitons dans ce chapitre aux systèmes à simple entrée et simple sortie~:

\begin{definition}{Système SISO (Single Input Single Output)}
  
  Un système discret SISO est une application qui associe à un signal
  d'entré $\x$ un signal de sortie unique $\y$. Les signaux $\x$ et $\y$
  sont des fonctions prises dans un espace le plus général possible noté
  ici $E$. Ce qui donne pour un système SISO discret nommé $G$~:
  \begin{equation}
    G : \qquad \application{E}{E}{\x}{\quad \y} 
    % DONE : correction TYPAGE et remarque notation
  \end{equation}

\end{definition}

Les signaux peuvent être des fonctions de la variable réelle continue
ou discrète, prenant des valeurs réelles, complexes voire
composite. La \tabref{tab:espaces_signaux} liste les principaux
espaces de signaux.

\begin{table}[!ht]
  \begin{tabular}{p{0.1\textwidth}|c|c}
   Espaces  & à variable continue    & à variable discrète  \\\hline
    fonction réelle & $E=\R^\R,\quad f\in E $  &  $E=\R^\Z,\quad f\in E $   \\
            & $ f:\application{\R}{\R}{t}{f(t)} $   &  $\application{\Z}{\R}{k}{f\b{k}} $   \\
    \hline
        fonction complexe & $E=\C^\R$   &    $E=\C^\Z$  \\
    & $f:\application{\R}{\C}{t}{f(t)} $   &    $f:\application{\Z}{\C}{k}{f\b{k}} $     \\
    \hline
        image réelle & $E=\R^{\R^2}$   &   $E=\R^{\Z^2}$\\     
    & $f:\application{\R^2}{\R}{\p{x,y}}{f(x,y)} $   &   $f:\application{\Z^2}{\R}{\p{l,k}}{f\b{l,k}}$      \\
    \hline
       image  couleur & $E={\R^3}^{\R^2}$   &   $E={\R^3}^{\Z^2}$      \\
    & $f:\application{\R^2}{\R^3}{\p{x,y}}{f\p{x,y}=\vvvect{r\p{x,y}}{v\p{x,y}}{b\p{x,y}}} $   &   $f:\application{\Z^2}{\R^3}{\p{l,k}}{f\b{l,k}=\vvvect{r\b{l,k}}{v\b{l,k}}{b\b{l,k}}}$      \\
\hline
  \end{tabular}
  \caption{Les différents espaces de signaux continus et discets de différents types de scalaires ($\R$ et $\C$) et différentes dimensions.}
  \label{tab:espaces_signaux}
\end{table}

\begin{remarque}
  On choisit des notations permettant de distinguer clairement~:
  \begin{itemize}
  \item un signal $\vec{f}$  et une variable $f$~;
  \item une valeur $f\p{\nu}$ d'un signal $f$ continu évalué à un instant $\nu\in\R$ et une valeur $f\b{\nu}$ pour un signal discret pris à l'instant $\nu\in\N$. Dans le cas d'une période d'échantillonnage notée $T_e$ on notera donc $f\b{k}=f\p{k.T_e}$~;
   \item une image de $x$ par l'application $f$ est notée $f\p{x}$ alors que l'image d'un signal $\x$ par l'opérateur $F$ est notée $F\OperDe{\x}$. Une application de la variable réelle ou entière est généralement appelée \emph{fonction} alors qu'une application de fonction en fonction est appelée \emph{opérateur}. 
  \end{itemize}
\end{remarque}

La vision par opérateur des systèmes consiste à représenter un système
$G$ comme une association de systèmes élémentaires. Ainsi nous
interprétons une fonction de transfert continue $G(p)$ comme \og{}un
système $G$ conçu par assemblage du système dérivateur $p$, de
l'intégrateur $p^{-1}$, de gains et de sommateurs\fg{}

En discret, le système $G$ sera représenté par une fonction de
transfert $G(z)$ représentant \og{}un système $G$ conçu à partir du
système $z$ (avance dans le temps d'un échantillon), de son inverse
$\zmu$ (retard d'un échantillon), de gains et de sommateurs\fg{}

\section{Systèmes linéaires~: gains et sommateurs}

La classe des systèmes linéaires est fondamentale car elle offre de
nombreux outils et propriétés mathématiques. Elle consiste à associer
des systèmes (linéaires) avec les opérateurs élémentaires gain et sommateur.

\begin{definition}{Système gain}
  \label{def:systeme_gain}
  
  Le système gain $\mathbf{\alpha}$ est un opérateur $E\to E$ qui consiste à multiplier l'entrée $\x\in E$ par un scalaire constant $\alpha$~:
  
  \begin{tabular}{m{0.3\textwidth}m{0.7\textwidth}}
%  \begin{minipage}[c]{\textwidth}
    \centerline\noindent\graphe{0.3\textwidth}{gain_symb} & $\x \mapsto \alpha\x\quad$ où $\alpha \in \R$ ou $\C$
%\end{minipage}
  \end{tabular}

  En continu la multiplication d'un signal $\x$ par un scalaire $\alpha$ est~:
  
  $\alpha.\x : \application{\R}{\R \text{ ou } \C}{t}{\alpha.x\p{t}} $

  
  En discret la multiplication donne~:
  
  $\alpha.\x : \application{\Z}{\R\text{ ou } \C}{k}{\alpha.x\b{k}} $

\end{definition}

Le sommateur est un opérateur binaire (qui prend deux signaux en entrée) et n'est donc pas un système SISO.

\begin{definition}{Système sommateur}
  \label{def:systeme_gain}
  
  Le système sommateur $+$ est un opérateur binaire $E^2\to E$ qui consiste à ajouter deux signaux d'entrée  $\xu,\xd\in E$ entre-eux~:
  
  \begin{tabular}{m{0.4\textwidth}m{0.6\textwidth}}
%  \begin{minipage}[c]{\textwidth}
    \centerline\noindent\graphe{0.4\textwidth}{sommateur_symb} & $(\xu,\xd) \mapsto \xu+\xd$
%\end{minipage}
  \end{tabular}

  En continu la somme de signaux $\xu$ et $\xd$ est le signal $\xu+\xd$~:
  
  $\xu+\xd : \application{\R}{\R \text{ ou } \C}{t}{x_1\p{t}+x_2\p{t}} $

  
  En discret la somme donne~:
  
  $\xu+\xd : \application{\Z}{\R \text{ ou } \C}{k}{x_1\b{k}+x_2\b{k}} $

\end{definition}

On remarque que ces opérateurs $+$ et $\alpha$ sont commutatifs,
associatifs, que le gain est distributifs sur la somme, etc.  par
héritage de l'algèbre des nombres scalaires usuels avec les opérateurs
$+$ et $\times$.

On peut donc parler de combinaison linéaire de signaux pour évoquer
une somme de signaux amplifiés comme
$\alpha_1.\xu+\alpha_2.\xd+\dots$. On peut donc extrapoler la
linéarité d'une application dans l'algèbre classique à la notion de
système linéaire.

\begin{definition}{Système linéaire}
  \label{def:linearite}


  Un système est dit linéaire si, et seulement si, l'application $G$
  associée est linéaire, soit pour tout
  $\p{\xu, \xd, \lambda} \in E^2 \times \R$~:
  \begin{eqnarray}
    \label{eq:def_linearite}
    G\OperDe{\lambda.\xu + \xd} = \lambda.G\OperDe{\xu} + G\OperDe{\xd}
  \end{eqnarray}

\centering\graphe{0.75\textwidth}{op_lineaire}

Autrement dit :

\og{} la sortie d'une combinaison d'entrées est la combinaison des
sorties de chaque entrée.\fg{}

\end{definition}



Cela correspond au théorème de superposition cher à l'électronicienne~: \og{} la réponse
d'un système électronique à une combinaison de générateurs est la combinaison des
réponses à chaque générateur.\fg{}


\begin{exemple}
  \label{exemple:differentiateur_lineaire}
  L'effet du système \emph{différentiateur} sur le signal d'entrée est
  d'écrit par l'opérateur L~:
  \begin{tabular}{m{0.6\textwidth}m{0.4\textwidth}}
  \graphe{0.6\textwidth}{differentiateur} &   $L : \x \mapsto \y=\frac{\x-\oret\OperDe{\x}}{T_e}$
  \end{tabular}

  L'opérateur T est un opérateur élémentaire des systèmes discrets qui
  consiste à retarder un signal d'un échantillon $T : \x\b{\bullet} \to \x\b{\bullet - 1}$. Vérifions que cet
  opérateur est linéaire~:

  \begin{eqnarray*}
    L\OperDe{\xu+\lambda \xd}\b{k} &= L\OperDe{\xu+\lambda \xd}\b{k}\\
                              &= \frac{x_1\b{k}+\lambda x_2\b{k} - \oret\OperDe{\xu+\lambda \xd}\b{k}}{T_e}\\
                              &= \frac{x_1\b{k}+\lambda x_2\b{k} - \p{x_1\b{k-1}+\lambda x_2\b{k-1}}}{T_e} = A\\
    L\OperDe{\xu}\b{k}+\lambda\,L\OperDe{\xd}\b{k}  &= \frac{x_1\b{k} - x_1\b{k-1}}{T_e}+\lambda\,\frac{x_2\b{k} - x_2\b{k-1}}{T_e}=A
  \end{eqnarray*}
  Le système est donc linéaire.
\end{exemple}


% \begin{remarque}
%   Pour résoudre les complexes équations différentielles des
%   télégraphistes, \Heaviside{} utilise ces opérateurs de base et
%   introduit le \emph{calcul opérationnel}. Cela consiste à
%   représenter l'application de l'opérateur dérivée sur une fonction
%   $f$ comme une simple multiplication par un nombre $p$. Ainsi une
%   équation différentielle $a\,y'' + 2y' -y = 3\int x$ est associée à
%   l'équation symbolique $a\,p^2\,y + 2\,p\,y - y = \frac{3}{p}x$. Il
%   est alors possible de résoudre algébriquement l'équation sous
%   forme de fractions rationnelles ce qui donnerait avec notre
%   exemple~: $y = \frac{3/p}{a\,p^2+2p+1} x$. Rappelons que $x$ et
%   $y$ sont des fonctions et non des réels et que dans ce cas les
%   opérations ne sont pas de simple multiplication et addition de
%   réels mais bien des multiplications et additions de fonctions. La
%   variable symbolique $p$ est utilisée comme un nombre réel mais
%   n'est en aucun cas un réel...
% \end{remarque}

\section{Systèmes élémentaires et opérateurs}
\def\d0{\vec{\delta_0}}

Dans cette section nous ajoutons aux opérateurs gain et sommateur un
opérateur élémentaire et son inverse ~:
\begin{itemize}
\item en dicret : l'avance unitaire d'un échnatillon donne la
  transformée en Z avec $z$ associé à l'avance unitaire, et son inverse $\zmu$ au retard unitaire d'un échantillon~;
\item le dérivateur en continu donne la transformée de Laplace avec $p$ associé au système dérivateur et son inverse $p^{-1}$ associé à l'intégrateur.
\end{itemize}

\subsection{Systèmes discrets élémentaires~: équations aux différences}
Un système discret est majoritairement réalisé de manière logicielle
où le signal d'entrée $\x$ est un tampon de mémoire contenant les
échantillons de valeurs $x\b{k}$ à traiter. Un calcul est effectué
périodiquement à chaque nouvel échantillon et la sortie $y[k]$ est
envoyée en sortie et stockée dans un tampon mémoire.

Ainsi on s'intéresse à un ensemble de systèmes discrets pouvants être
réalisés par une équation aux différences (récurrence) de la forme~:
\begin{eqnarray}
  \label{eq:systeme_recurrence}
   a_0.y\b{k} \,+\,  a_1.y\b{k-1} \,+\,  \dots \,+\, a_n.y\b{k-n}     \;=\;  b_0.x\b{k} \,+\, \dots \,+\,b_m.x\b{k-m}  \quad,\quad \forall k\in\Z
\end{eqnarray}


On remarque qu'une récurrence mets en oeuvre des systèmes gain
(produit par $a_i$ et $b_i$)~; des systèmes sommateurs~; et des
opérateur élémentaire de retard unitaire $T$ et son inverse (celui
d'avance unitaire) noté $T^{-1}$.

La récurrence vue par composition d'opérateurs élémentaires devient ainsi~:

\begin{equation}
  \begin{array}{llllllll}
  a_0.\y{} \,&+\, a_1.\caCest{{\oret}\OperDe{\y}}{k\mapsto y\b{k-1}} \,&+\,\dots \,&+\,a_n.\caCest{\overbrace{\oret\circ\dots\circ \oret}^{\text{n fois}}\OperDe{\y}}{k\mapsto y\b{k-n}}\;&= \quad b_0.\x{} \,&+\, \dots \,&+\, b_m\,\circ\,\overbrace{\oret\circ\dots\circ \oret}^{\text{m fois}}\OperDe{\x}  & \nonumber \\
a_0.\y{} \,&+\, {a_1.\oret}\OperDe{\y} \,&+\, \dots \,&+\, a_n.\oret^n\OperDe{\y}   &= \quad  b_0.\x{} \,&+\, \dots \,&+\,{b_m.\oret^m}\OperDe{\x} \nonumber \\
  (\quad a_0 \,&+\, {a_1.\oret} \,&+\,\dots \,&+\, a_n.\oret^n \quad)\OperDe{\y} &=\quad (b_0 \,&+\, \dots \,&+\,  {b_m.\oret^m} \quad ) \OperDe{\x} \nonumber \\ &&&&&&&
  \end{array}
\end{equation}

On peut  reformuler la récurrence pour imposer $a_0=1$, sans perte de
généralité, et mettre en évidence la forme récursive $\y$ en fonction
d'une combinaison linéaire de l'entrée $\x$ et de ses valeurs précédentes (MA --
Mooving Average -- Moyenne glissante) et d'une combinaison linéaire
des valeurs de la sortie calculées précédemment (AR -- Auto-Regressive --
Auto-regression)~:

\begin{equation}
  \begin{array}{lll}
\caCest{a_0}{=1}.y\b{k} \;&= \caCest{ b_0.x\b{k} \,+\, \dots \,+\, b_m.x\b{k-m}}{\text{MA--Moyenne glissante}} \;& \caCest{-\,  a_1.y\b{k-1}  \dots  \,-\, a_n.y\b{k-n}}{\text{AR--Auto-régression}}   \quad \forall k\in\Z \nonumber\\
\y{} \;&= \overbrace{(\quad b_0 \,+\, \dots \,+\, {b_m.\oret^m} \quad )\OperDe{\x}}^{} \,&-\, \overbrace{(\quad{a_1.\oret} \,+\, \dots \,+\, a_n.\oret^n\quad)\OperDe{\y}}^{}   \nonumber \\ 
  \end{array}
\end{equation}

Les systèmes discrets peuvent ainsi se représenter sous la forme générale de schéma bloc~:


\graphe{0.9\textwidth}{bloc_ma_ar}

On définit dans la suite la transformée en Z d'un signal associant
l'opérateur $\oret$ à la multiplication par $\zmu$.  En ajoutant la
notion d'inverse d'un système, nous obtenons ainsi la fonction de
transfert d'un système discret $G\p{z}$ signifiant le système $G$
conçu par combinaison de l'opérateur avance unitaire $z$ et son
inverse~: le retard unitaire $T$.

\begin{equation}
  \begin{array}{lll}
    \left( a_0 \,+\, {a_1.z} \,+\,\dots \,+\, a_n.z^n \right).&Y(z) \quad=\quad \left(b_0 \,+\, \dots \,+\,  {b_m.z^m} \right) &.X(z) \nonumber \\
                 & Y(z)\quad=\quad\caCest{\frac{b_0 \,+\, \dots \,+\,  {b_m.z^m}}{a_0 \,+\, {a_1.z} \,+\,\dots \,+\, a_n.z^n}}{G(z)} &.X(z)
  \end{array}
\end{equation}


La \tabref{tab:op_discrets} résume les opérateurs élémentaires utilisés en discret en y ajoutant~:
\begin{description}
\item[la transformée en Z :] définie par la suite en associant $\zmu$ et l'opérateur retard unitaire $\oret$.
\item[la réponse impulsionnelle~:] définie comme la réponse du système à une impulsion unité noté $\d0$.
\item[la réponse harmonique~:] représentée ici comme un phaseur
  $H(f)\in\C$ se multipliant à une onde pure complexe
  $k\mapsto e^{i.2\pi.\tilde{f}.k}$ en entrée pour donner l'amplitude
  et la phase de l'onde pure en sortie. La représentation est ici
  faite pour une seule fréquence normalisée
  $\tilde{f}=\frac{f}{F_e}$~; le diagramme de Bode étant la
  représentation la plus connue de cette réponse harmonique permettant
  de visualiser ce phaseur pour toutes les fréquences.
\end{description}

\begin{table}[!ht]
  \centering
  \begin{tabular}[c]{|c|c|c|c|c|}
    \hline
    Système & Opérateur / & Transformée  & Réponse  & Réponse  \\
            & Récurrence  &   en $\TZ$   & impulsionnelle & harmonique \\\hline
    Gain & $\y = \alpha. \x$ & $Y[z]=\alpha.X[z]$  & $\h=\alpha.\d0$ & $H(f)= \alpha$ \\
   \hspace{-1cm}\graphe{0.3\textwidth}{gain_symb}\hspace{-1cm}         &$y[k]=\alpha.x[k]$ & & \graphe{0.25\textwidth}{gain_rip}& \hspace{-1cm}\graphe{0.25\textwidth}{gain_phaseur}\hspace{-1cm} \\\hline
    Retard & $\y = \oret\OperDe{\x}$ & $Y[z]=z^{-1}.X[z]$  & $\h=\vec{\delta_1}$ & $H(f)=e^{-i.2\pi.\tilde{f}}=e^{-i\Delta}$ \\
      \hspace{-1cm}\graphe{0.3\textwidth}{retard_symb}\hspace{-1cm}   &$y[k]=x[k-1]$ & & \graphe{0.25\textwidth}{retard_rip} & \graphe{0.25\textwidth}{retard_phaseur}\hspace{-1cm}\\\hline
    Avance & $\y = \oret^{-1}\OperDe{\x}$ & $Y[z]=z.X[z]$  & $\h=\vec{\delta_{-1}}$ & $H(f)= e^{+i.2\pi.\tilde{f}}=e^{+i\Delta}$   \\
      \hspace{-1cm}\graphe{0.3\textwidth}{avance_symb}\hspace{-1cm}   &$y[k]=x[k+1]$ & & \graphe{0.25\textwidth}{avance_rip} &  \hspace{-1cm}\graphe{0.25\textwidth}{avance_phaseur}\hspace{-1cm} \\\hline    
  \end{tabular}
  \caption{Opérateurs discrets élémentaires}
  \label{tab:op_discrets}
\end{table}


\subsection{Systèmes continus élémentaires~: équations différentielles}
En continu, comme en électronique analogique par exemple, on associe des composants électriques ayant une relation entre courant et tension qui est~:
\begin{description}
\item[Gain -- Résistance] où la relation est linéaire $U=R.I$~;
\item[Intégrateur -- Condensateur] où la relation est une intégrale $U=\frac{1}{C}.\int i(t).dt$~;
\item[Dérivateur -- Inductance] où la relation est celle de la dérivée $U=L.\frac{d}{dt}.i(t)$~;
\end{description}

L'assemblage de ces fonctions n'est pas aussi immédiate qu'en discret
et nécessite un savoir-faire d'électronicienne. Une méthode coûteuse
mais méthodique consiste à utiliser des amplifcateurs opérationnels
pour réaliser les opérateurs (intégrateur, sommateur, etc.) et pouvoir
les associer sans problème d'adaptation d'impédance~: d'où le termel
\og{}opérationnel\fg{} qui signifie amplificateur permettant de
réaliser et d'assembler les opérations sur les signaux.

Ainsi on s'intéresse à un ensemble de systèmes continus pouvants être
réalisés par une équation différentielle de la forme~:
\begin{eqnarray}
  \label{eq:systeme_equa_diff}
   a_0.y\p{t} \,+\,  a_1.y' \,+\,  \dots \,+\, a_n.y^{\b{n}}     \;=\;  b_0.x\p{t} \,+\, \dots \,+\,b_m.x^{\b{m}}  \quad,\quad \forall t\in\R
\end{eqnarray}


De même une équation différentielle mets en oeuvre des systèmes gain
(produit par $a_i$ et $b_i$)~; des systèmes sommateurs~; et des
opérateur élémentaire de dérivée $\oder$ et son inverse (celui
d'intégrale ) noté $\oint = D^{-1}$.

L'équation différentielle vue par composition d'opérateurs élémentaires devient ainsi~:

\begin{equation}
  \begin{array}{llllllll}
a_0.\y{} \,&+\, {a_1.\oder}\OperDe{\y} \,&+\, \dots \,&+\, a_n.\oder^n\OperDe{\y}   &= \quad  b_0.\x{} \,&+\, \dots \,&+\,{b_m.\oder^m}\OperDe{\x} \nonumber \\
  (\quad a_0 \,&+\, {a_1.\oder} \,&+\,\dots \,&+\, a_n.\oder^n \quad)\OperDe{\y} &=\quad (b_0 \,&+\, \dots \,&+\,  {b_m.\oder^m} \quad ) \OperDe{\x} \nonumber \\ &&&&&&&
  \end{array}
\end{equation}

Il est aisé de voir qu'avec la transformée de Laplace l'opérateur
$\oder$ revient à multiplier par $p$, en ajoutant la notion d'inverse
d'un système nous obtenons ainsi la fonction de transfert d'un système
$G\p{p}$ signifiant le système $G$ conçu par combinaison de l'opérateur $p$ et son inverse.

\begin{equation}
  \begin{array}{lll}
    \left( a_0 \,+\, {a_1.p} \,+\,\dots \,+\, a_n.p^n \right).&Y(p) \quad=\quad \left(b_0 \,+\, \dots \,+\,  {b_m.p^m} \right) &.X(p) \nonumber \\
                 & Y(p)\quad=\quad\caCest{\frac{b_0 \,+\, \dots \,+\,  {b_m.p^m}}{a_0 \,+\, {a_1.p} \,+\,\dots \,+\, a_n.p^n}}{G(p)} &.X(p)
  \end{array}
\end{equation}


\section{Systèmes invariants}
Il est fréquent, et surtout théoriquement utile, qu'un système
réagisse de la même manière indépendemment de l'instant où est
appliqué le signal d'entrée. Ce qui conduit à la définition suivante~:

\begin{definition}{Système invariant dans le temps}

  Un système discret (resp. continu) est dit invariant dans le temps
  si et seulement si son application associée $G$ 
  vérifie~:
  \begin{eqnarray}
    \forall x\in E, \forall k_0\in \Z, \quads G\OperDe{\x\b{\bullet-k_{0}}} = G\OperDe{\x}\b{\bullet-k_{0}} \\
    \forall x\in E, \forall t_0\in \R, \quads G\OperDe{\x\p{\bullet-t_{0}}} = G\OperDe{\x}\p{\bullet-t_{0}} \nonumber
  \end{eqnarray}
  
  En terme d'opérateur~; un système $G$ est invariant dans le temps
  si, et seulement si, son opérateur commute avec tout opérateur
  retard de $k_0$ (resp. $\tau_0$ en continu) noté $\oretDe{k_0}=\oret^{k_0}~: \x \mapsto \x\b{\bullet-k_0}$~:
  \begin{eqnarray}
    \label{eq:sys_invariant}
    G\circ\oretDe{k_0} \; = \; \oretDe{k_0}\circ G  \qquad \iff \qquad  G\OperDe{\oretDe{k_0}\OperDe{\x}} = \oretDe{k_0}\OperDe{G\OperDe{\x}}
    \\
    G\circ\oretDe{\tau_0} \; = \; \oretDe{\tau_0}\circ G  \qquad \iff \qquad  G\OperDe{\oretDe{\tau_0}\OperDe{\x}} = \oretDe{\tau_0}\OperDe{G\OperDe{\x}}\nonumber
  \end{eqnarray}

  \centering\graphe{0.5\textwidth}{op_invariant}

  Autrement dit

  \og{}la réponse du système à un signal retardé est le
  retard de la réponse du système.\fg{}~;

  ou encore

  \og{} la réponse du
système ne dépend pas de l'origine des temps choisie.\fg{}
\end{definition}



%% TODO lien vers causalité.

\begin{remarque}
  Il est facile de vérifier que les systèmes discrets élémentaires que
  sont le gain~; le retard unitaire et l'avance unitaire (gain,
  dérivateur et intégrateur pour le continu) sont invariants.  Il en
  est de même pour tout système constitué de combinaisons linéaires et
  de composition de systèmes élémentaires.
  
  Il suffit alors de montrer que le système se décompose avec des
  \emph{coefficients constants} avec des systèmes élémentaires en le
  mettant sous forme \emph{d'équation aux différences} ou
  \emph{récurrence} à coefficients constants (\emph{equations
    différentielle} en continu) ou en un schéma bloc à coefficients
  constant.
\end{remarque}


\section{Réponses d'un système LTI}
Rappelons qu'un système peut aussi bien être un filtre, un modèle d'un processus à commander, un correcteur dans une boucle d'asservissement, le modèle d'une boucle fermée, etc.~: calculer la
réponse d'un système à un signal, c'est aussi \og{}faire du
filtrage.\fg{}

La première section utilise la propriété de linéarité associée à la
notion de réponse harmonique pour obtenir la réponse fréquentielle
d'un système $H$ à n'importe quel signal d'entrée $\x$ en le
décomposant en somme d'ondes pures. On obtient ainsi la relation de
filtrage entre les spectres des signaux~
$\hat{Y}\p{f}=H\p{f}.\hat{X}\p{f}$.

En un second temps nous utiliserons la propriété de linéarité, la
propriété d'invariance, et la notion de réponse impulsionnelle $\h$
d'un système $H$ pour obtenir le signal de sortie pour n'importe quel
signal d'entrée. On obtient ainsi le relation temporelle de
convolution~$\y = \x\conv\h$ illustrée ci-dessous.

\graphe{0.6\textwidth}{reponse_temporelle_bref}

\subsection{Réponse temporelle : réponse impulsionnelle et convolution}
\label{sec:rip}
\def\dj{\vec{\delta_j}}
Dans le cas discret, l'obtention de la réponse d'un système est facile
à obtenir en utilisant les propriétés LTI du système $H$ et en
décomposant le signal d'entré $\x$ dans la base temporelle canonique.


Contrairement au cas continu, un signal d'entrée quelconque $\x$ se
décompose facilement en impulsions unité (voir la base temporelle de
$\RN$ dans \secref{sec:RN}). La \figref{fig:reponse_convolution}
reprend la décomposition en impulsions d'un signal d'entrée
$\x=\somme{j\in\Z}{}{x[j].\vec{\delta_0[\bullet-j]}}$ et applique un système
$H$ de réponse impulsionnelle $\h$ à chaque impulsion pour obtenir la
réponse $\y$ du système.

\begin{figure}[ht!]
  \centering \graphe{0.9\textwidth}{reponse_convolution}
  \caption{La réponse d'un système linéaire invariant $y$ est la somme
    des réponses impulsionnelles des impulsions qui composent le
    signal d'entré $s$.}
  \label{fig:reponse_convolution}
\end{figure}

On retrouve ainsi la formule de convolution temporelle de
$\y=\x\conv \h$. Le résultat est démontré dans l'équation
\eqref{eq:reponse_convolution} en utilisant les propriétés de
linéarité et invariance du système.


\begin{align}
  \label{eq:reponse_convolution}
  \caCest{\x = \somme{j=-\infty}{\infty}{x\b{j}.\overbrace{\dj}^{k\mapsto \delta_0\b{k-j}}}}{\text{signal}}\quad \implies\quad &\y=H\OperDe{\x}=H\OperDe{\somme{j=-\infty}{\infty}{x\b{j}.\dj}} \nonumber\\
  \underset{\text{linéarité de H}}{\implies}\quad &\y=\somme{j=-\infty}{\infty}{H\OperDe{ \;\caCest{x\!\!\b{j}}{\text{constante !}}.\caCest{\dj}{\text{fonction de k}}\;}} \\
  & \quad\underset{\text{linéarite de H}}{=} \;\;\somme{j=-\infty}{\infty}{x\!\b{j}.H\OperDe{\caCest{\dj}{\oret^j\OperDe{\d0}}}} \nonumber \\
  \implies \quad &\y=\somme{j=-\infty}{\infty}{x\b{j}.H\OperDe{\oret^j\!\OperDe{\delta_{0}}}} \\
  &\quad \underset{\text{invariance de H}}{=} \;\;\somme{j=-\infty}{\infty}{x\b{j}.\oret^j\OperDe{\caCest{H\OperDe{\d0}}{\h}}}\nonumber\\
  \implies \quad & \y= \somme{j=-\infty}{\infty}{x\b{j}.\caCest{\oret^j\OperDe{\h}}{k\mapsto h\b{k-j}}}=\x\conv \h \nonumber\\
  \iff \quad & \forall k\in\Z,\quad  y\b{k}= \somme{j=-\infty}{\infty}{x\b{j}\,h\!\b{k-j}} = \x\conv \h\b{k}
\end{align}
où l'opérateur $\conv$ est la convolution discrète définie ci-après.

\begin{definition}{Convolution discrète}
  \label{def:convolution_discrete}
  
  La convolution de signaux discrets $\vec{u}$ et $\vec{v}$ notée $\vec{u}\conv\!\vec{v}$ est le
  signal discret défini par~:
  \begin{align}
    \label{eq:convolution_discrete}
    \vec{u}\conv\!\vec{v} \quad : \quad k \mapsto \somme{j=-\infty}{\infty}{u\ded{j}.v\ded{k-j}} \\
    \vec{u}\conv\!\vec{v} = \somme{j=-\infty}{\infty}{u\ded{j}.\vec{v}\ded{\bullet-j}} \nonumber
  \end{align}
\end{definition}


L'analogie avec la convolution de signaux continus fonctionne aussi
mais fait appel à une décomposition en impulsions de \Dirac{}
théoriquement discutable~:
$\x =\int\limits_{\R}^{}{x(\tau).\vec{\delta_0\p{\bullet-\tau}}d\tau}$.
 On obtient ainsi la même formule sous forme de somme de réponses impulsionnelles retardées~:

\begin{definition}{Convolution continue}
  \begin{align}
    \label{eq:convolution_continue}
    \vec{u}\conv\!\vec{v} \quad :& \quad t\mapsto \integ{\tau=-\infty}{\infty}{u\de{\tau}.v\de{t-\tau}.d\tau} \\
    & \vec{u}\conv\!\vec{v} = \integ{\tau=-\infty}{\infty}{u\de{\tau}.\vec{v\de{\bullet-\tau}}.d\tau}
  \end{align} 
\end{definition}


\subsection{Réponse harmonique~: filtrage}



\section{Stabilité d'un système LTI}
\label{sec:stabilite}
La réponse impulsionnelle (notée \RIP) caractérise entièrement un
système LTI, puisque l'opération de convolution permet d'obtenir la
sortie d'un système pour n'importe quelle entrée.

Nous alons caractériser deux types de stabilité des systèmes LTI à
partir de leur \RIP{}~:
\begin{description}
\item[Stabilité BIBO] (\emph{Borned Imput Borned Output}, il n'y pas
  de sigle Français d'usage courant) associée à la propriété de donner
  des réponses bornées pour toutes entrée bornée~;
\item[Stabilité simple] chère aux automaticiennes qui exprime le fait
  qu'un système relâché (dont l'entrée reste nulle à partir d'un
  instant) retourne vers son état d'équilibre qui, dans le cas de
  systèmes linéaires, correspond à l'état null et à une sortie nulle.
\end{description}

\subsection{Stabilité BIBO}

En pratique, la stabilité BIBO garantit qu'un système numérique puisse
toujours calculer une valeur finie et donc codable à partir d'un
signal réel forcément borné.

\begin{remarque}\remarqueTitre{Notations}
  On rappelle qu'un signal $s$ est borné ssi~:
  $$\exists A\in\R \tq{} \forall k\in \Z, \quad \abs{s\ded{k}}\leq A $$

  On note de manière concise $\abs{x}\leq A$, cette proposition.

  Un signal non borné est, par négation de la proposition, tel que~:
  $$\forall A\in\R, \exists k\in \Z \tq  \abs{s\ded{k}} > A $$

  Cela revient à dire que la suite $\p{s\ded{k}}_{k\in\Z}$ diverge
  vers l'infini (ne pas confondre avec simplement divergente qui est
  la négation de convergente~; par exemple $k\mapsto \sin\ded{k}$ est
  divergente mais non divergente vers l'infini).

  On notera de manière concise $s\to\infty$ le fait que $s$ soit
  non-borné.
  
\end{remarque}

\begin{definition} Un système $H$ est \textbf{stable en entrée bornée
    / sortie bornée} ou \textbf{stable BIBO} ssi pour toute entrée $x$
  bornée la réponse $y=H\b{x}$ du système est bornée~:

  $$\forall x, \quad \abs{x}\leq A \quad\implies\quad \abs{y}\leq B $$
\end{definition}


\begin{theoreme}
  Un système $H$ est stable BIBO si, et seulement si, sa réponse
  impulsionnelle $k\mapsto h\b{k}$ est de module sommable, c.-à-d., la
  série $\left\{h\b{k}\right\}_{k\in\Z}$ est absolument convergente~:
  $$ H \text{ stable BIBO} \iff\sum\limits_{k\in\Z} |h\ded{k}| \in \R$$
\end{theoreme}

La démonstration est intéressante et simple~:

\begin{demo} --- Stabilité BIBO

  
  Nous démontrons l'équivalence pour tout système $H$ de réponse
  impulsionnelle $h$~:
  $$ \sum \abs{h}\in\R \iff \text{stable BIBO} \overset{\Delta}{\iff} \p{\forall x, \;\;\abs{x}<A \implies \abs{y}<B}$$  par
  double implication.
  
  \paragraph{\underline{$\implies$~:}}
  Supposons que $\sum\limits_{k\in\Z} \left|h\ded{k}\right|=C\in\R^+$
  et que $\abs{x}\leq A$~; et montrons que $\abs{y}\leq B$.

  Si l'on écrit la réponse $y$ du système à ce $x$ borné en utilisant
  la convolution \eqref{eq:convolution} et que l'on majore les termes
  par leurs valeurs absolues~:

  \begin{align*}
    y=x\conv{}h &: k\mapsto \somme{j=-\infty}{\infty}{h\ded{j}.x\ded{k-j}} \\
    \forall k\in\Z,\quad y\ded{k} & \leq \somme{j=-\infty}{\infty}{\left|h\ded{j}.x\ded{k-j}\right|} = \somme{j=-\infty}{\infty}{|h\ded{j}|.|x\ded{k-j}|}\\
    \implies  y\ded{k} & \leq \somme{j=-\infty}{\infty}{|h\ded{j}| . A}\quad \text{car x est bornée par A}\\
    \implies  y\ded{k} & \leq \;A.\somme{j=-\infty}{\infty}{|h\ded{j}|} = A.C \in \R^+\quad \text{car h est absoluement sommable}\\
    \implies & \quad\exists B=A.C\in\R^+ \mathrel{}\mid\mathrel{}\quad \forall k\in\Z, \; |y\ded{k}|\leq B \implies \abs{y}\leq B
  \end{align*}

  \paragraph{\underline{$\impliedby$ :}}
  La réciproque est plus astucieuse et s'exprime par~:
  $$\forall h,\quad \underbrace{\p{\forall x,\; \abs{x}\leq A \implies \abs{x\conv h}\leq B}}_{P} \implies \underbrace{\sum|h|\in\R}_{Q}$$
  Il est plus aisé de montrer la contra-posée
  $\overline{Q}\implies\overline{P}$ et donc de supposer que
  $\sum|h|\; DV$ et montrer que $\overline{P}$~:

  $$\exists x \tq \abs{x}\leq A \text{ et } \abs{x\conv h}\to\infty $$
  
  Pour cela nous construisons un signal $x$ tel que, de manière
  arbitraire, la sortie $y$ soit non-bornée pour $k=0$ et valle
  $\sum\abs{h}$ et donc diverge (par hypothèse sur $h$).
  
  En prenant la formule de convolution pour $k=0$, nous cherchons
  $x$ tel que~:
  $$ x\conv h\ded{0}= \somme{j=-\infty}{\infty}{x\ded{j}.h\ded{0-j}} =  \somme{j=-\infty}{\infty}{|h\ded{j}|} = +\infty$$
  
  On montre donc que ce signal existe en prenant
  $x[j]=\frac{\overline{h\ded{-j}}}{\abs{h\ded{-j}}}$ et si
  $h\ded{-j}=0$ alors nous prenons $x\ded{j}=0$.  Ce qui permet de
  conclure la réciproque avec~:
  
  $$\exists x=\frac{\overline{h\ded{-j}}}{\abs{h\ded{-j}}} \tq x\conv h = \somme{j=-\infty}{\infty}{\frac{\overline{h\ded{-j}}}{\abs{h\ded{-j}}}.h\ded{0-j}}= \somme{j=-\infty}{\infty}{\abs{h\ded{-j}}}=+\infty$$

  L'implication et sa réciproque sont donc vraies~: CDFQ.
  
\end{demo}


\section{Stabilité simple}
Lorsque le système discret représente le comportement d'un processus,
l'automaticienne préfère la notion de stabilité simple qui évoque le
fait qu'un système libre (entrée nulle) retourne vers son état
d'équilibre (valeur nulle pour les systèmes linéaires).

\begin{definition} Un système LTI est dit \textbf{simplement stable}
  si pour toute entrée $x$ relâchée (nulle à partir d'un certain rang
  $k_0$) sa réponse tends vers 0 en $+\infty$.

  $$H \text{ stable}\quad\quad \overset{\Delta}{\iff}\quad \quad\forall x,\; \exists k_0 \tq{} \forall k>k_0, \; x\ded{k}=0 \quad\implies\quad y\ded{k} \underset{k\to+\infty}{\longrightarrow} 0  $$
\end{definition}

La encore, on peut déduire cette propriété de stabilité à partir de la
réponse impulsionnelle.

\begin{theoreme}
  Un système LTI est stable simplement si, et seulement si, sa réponse
  impulsionnelle $k\mapsto h\b{k}$ tend vers 0 lorsque $k\to +\infty$.
  
  $$ H \text{ simplement stable} \iff \lim_{k\to+\infty} h\ded{k} = 0 $$
\end{theoreme}


La démonstration se fait en utilisant le produit de convolution et en
passant à la limite~: elle est laissée aux soins de la lectrice.






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "poly_discret"
%%% End:

